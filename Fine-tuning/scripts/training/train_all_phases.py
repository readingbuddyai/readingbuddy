#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Wav2Vec2-XLS-R-300M ì–´ë¦°ì´ ìŒì„± íŒŒì¸íŠœë‹ - ì „ì²´ Phase í†µí•©
Phase 1 (30h) â†’ Phase 2 (90h) â†’ Phase 3 (148h) â†’ Phase 4 (ì„ íƒ)
LoRA + í•˜ìœ„ì¸µ ë™ê²° + Curriculum Learning
"""

import os
import sys
import json
import argparse
import glob
import numpy as np
import torch
import soundfile as sf
from dataclasses import dataclass
from typing import Dict, List, Union, Optional
from pathlib import Path
from torch.utils.data import IterableDataset, DataLoader
from transformers import (
    Wav2Vec2FeatureExtractor,
    Wav2Vec2Processor,
    Wav2Vec2ForCTC,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    TrainerCallback
)
from peft import LoraConfig, get_peft_model, TaskType
from datetime import datetime

# í•œêµ­ì–´ G2P (ê¸°ì¡´ íŒŒì¼ì—ì„œ import)
sys.path.append('/home/j-k13a206/finetunning')
from korean_g2p import KoreanG2P


# =====================
# Phase ì„¤ì •
# =====================
PHASE_CONFIG = {
    1: {
        'name': 'phase1_30h',
        'train_ratio': 0.20,  # Trainì˜ 20%
        'epochs': 10,
        'learning_rate': 5e-4,
        'warmup_steps': 500,
        'target_wer': 0.25,
        'augmentation_strength': 'weak',
    },
    2: {
        'name': 'phase2_90h',
        'train_ratio': 0.60,  # Trainì˜ 60%
        'epochs': 8,
        'learning_rate': 3e-4,
        'warmup_steps': 1000,
        'target_wer': 0.18,
        'augmentation_strength': 'strong',
    },
    3: {
        'name': 'phase3_148h',
        'train_ratio': 1.0,  # Train 100%
        'epochs': 5,
        'learning_rate': 1e-4,
        'warmup_steps': 500,
        'target_wer': 0.15,
        'augmentation_strength': 'medium',
    },
    4: {
        'name': 'phase4_full_finetune',
        'train_ratio': 1.0,
        'epochs': 2,
        'learning_rate': 5e-5,
        'warmup_steps': 100,
        'target_wer': 0.14,
        'augmentation_strength': 'none',
        'unfreeze_all': True,  # ì „ì²´ ëª¨ë¸ í•´ë™
    }
}

# ê³µí†µ ì„¤ì •
COMMON_CONFIG = {
    'gpu': '3',
    'batch_size': 16,
    'gradient_accumulation_steps': 1,
    'weight_decay': 0.01,
    'gradient_clip': 1.0,
    'num_workers': 0,  # IterableDatasetì€ single-processê°€ ë” ì•ˆì •ì 
    'prefetch_factor': None,
    'pin_memory': False,

    # ê²½ë¡œ
    'data_dir': '/home/j-k13a206/data/child_subset_100h',
    'model_path': '/home/j-k13a206/models/wav2vec2-korean-phoneme',
    'output_base_dir': '/home/j-k13a206/fine_tunining_new/checkpoints',

    # LoRA ì„¤ì •
    'lora_r': 16,
    'lora_alpha': 32,
    'lora_dropout': 0.1,
    'freeze_layers': list(range(0, 8)),  # 0~7ì¸µ ë™ê²°

    # Early Stopping
    'early_stopping_patience': 3,
    'early_stopping_threshold': 0.005,
}


# =====================
# IterableDataset (On-the-fly loading)
# =====================
class StreamingChildSpeechDataset(IterableDataset):
    """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ë¡œë”©"""

    def __init__(
        self,
        data_dir: str,
        split: str,  # "1.Training", "2.Validation_split", "3.Test"
        processor: Wav2Vec2Processor,
        g2p: KoreanG2P,
        max_samples: Optional[int] = None,
        train_ratio: float = 1.0,  # Phaseë³„ ì‚¬ìš© ë¹„ìœ¨
    ):
        self.data_dir = Path(data_dir)
        self.split = split
        self.processor = processor
        self.g2p = g2p
        self.max_samples = max_samples
        self.train_ratio = train_ratio

        # JSON íŒŒì¼ ìˆ˜ì§‘
        split_dir = self.data_dir / split
        json_pattern = str(split_dir / "**/*.json")
        self.json_files = sorted(glob.glob(json_pattern, recursive=True))

        # Trainì¸ ê²½ìš° ë¹„ìœ¨ë§Œí¼ë§Œ ì‚¬ìš©
        if "Training" in split and train_ratio < 1.0:
            n_samples = int(len(self.json_files) * train_ratio)
            self.json_files = self.json_files[:n_samples]

        if self.max_samples:
            self.json_files = self.json_files[:self.max_samples]

        print(f"[{split}] {len(self.json_files):,}ê°œ ìƒ˜í”Œ ë¡œë“œ (ratio: {train_ratio:.1%})")

        self.stats = {
            'success': 0,
            'riff_errors': 0,
            'too_short': 0,
            'invalid_audio': 0,
            'empty_text': 0,
            'empty_phonemes': 0,
            'processing_errors': 0,
            'other_errors': 0,
        }

    def __iter__(self):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìƒ˜í”Œ ìƒì„±"""
        for json_path in self.json_files:
            try:
                sample = self._load_sample(json_path)
                if sample is not None:
                    self.stats['success'] += 1
                    yield sample
            except Exception as e:
                self.stats['other_errors'] += 1
                continue

    def _load_sample(self, json_path: str) -> Optional[Dict]:
        """ë‹¨ì¼ ìƒ˜í”Œ ë¡œë”©"""
        try:
            # JSON ë¡œë“œ
            with open(json_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)

            # WAV ê²½ë¡œ êµ¬ì„±
            filename = metadata['File']['FileName']
            json_path_obj = Path(json_path)

            # ë¼ë²¨ë§ë°ì´í„° â†’ ì›ì²œë°ì´í„°ë¡œ ê²½ë¡œ ë³€í™˜
            parts = json_path_obj.parts
            try:
                labeling_idx = parts.index("ë¼ë²¨ë§ë°ì´í„°")
                relative_parts = parts[labeling_idx + 1:]
            except ValueError:
                self.stats['other_errors'] += 1
                return None

            wav_path = self.data_dir / self.split / "ì›ì²œë°ì´í„°" / Path(*relative_parts[:-1]) / filename

            if not wav_path.exists():
                self.stats['other_errors'] += 1
                return None

            # ì˜¤ë””ì˜¤ ë¡œë“œ
            try:
                speech, sr = sf.read(str(wav_path))
            except Exception as e:
                if 'RIFF' in str(e):
                    self.stats['riff_errors'] += 1
                else:
                    self.stats['invalid_audio'] += 1
                return None

            # ë„ˆë¬´ ì§§ì€ ì˜¤ë””ì˜¤ í•„í„°ë§
            if len(speech) < 1600:  # 0.1ì´ˆ
                self.stats['too_short'] += 1
                return None

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = metadata['Transcription']['LabelText'].strip()
            if not text:
                self.stats['empty_text'] += 1
                return None

            # ìŒì†Œ ë³€í™˜
            phonemes = self.g2p.text_to_phonemes(text)
            if not phonemes:
                self.stats['empty_phonemes'] += 1
                return None

            # Feature ì¶”ì¶œ
            input_values = self.processor(
                speech,
                sampling_rate=16000,
                return_tensors="pt",
                padding=False
            ).input_values.squeeze(0)

            # ë ˆì´ë¸” ì¸ì½”ë”© (tokenizer ì§ì ‘ ì‚¬ìš©)
            labels = self.processor.tokenizer(phonemes).input_ids

            return {
                'input_values': input_values,
                'labels': labels,
            }

        except Exception as e:
            self.stats['processing_errors'] += 1
            return None


@dataclass
class DataCollatorCTCWithPadding:
    """CTCìš© Data Collator"""
    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True

    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:
        # ë””ë²„ê¹…: features í™•ì¸
        if not features:
            raise ValueError(f"featuresê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤! features={features}")

        # Noneê³¼ ë¹ˆ dict í•„í„°ë§
        orig_len = len(features)
        features = [f for f in features if f is not None and f != {} and isinstance(f, dict)]

        if not features:
            raise ValueError(f"ëª¨ë“  featuresê°€ None ë˜ëŠ” ë¹ˆ dictì…ë‹ˆë‹¤! (ì›ë³¸ {orig_len}ê°œ)")

        # featuresì—ì„œ í•„ìš”í•œ í‚¤ë§Œ ì¶”ì¶œ
        input_values = [f["input_values"] for f in features if "input_values" in f]
        labels = [f["labels"] for f in features if "labels" in f]

        if not input_values or not labels:
            # ë””ë²„ê¹…: ì–´ë–¤ featureê°€ ë¬¸ì œì¸ì§€ í™•ì¸
            feature_keys_list = [list(f.keys()) for f in features[:5]]
            raise ValueError(f"input_values ë˜ëŠ” labelsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. features={len(features)}, input_values={len(input_values)}, labels={len(labels)}, feature_keys_sample={feature_keys_list}")

        # input_values íŒ¨ë”©
        max_length = max(len(x) for x in input_values)
        padded_inputs = []
        for x in input_values:
            padding_length = max_length - len(x)
            padded = torch.nn.functional.pad(x, (0, padding_length), value=0.0)
            padded_inputs.append(padded)
        batch_input_values = torch.stack(padded_inputs)

        # labels íŒ¨ë”©
        max_label_length = max(len(l) for l in labels)
        padded_labels = []
        for l in labels:
            padding_length = max_label_length - len(l)
            padded = l + [-100] * padding_length
            padded_labels.append(padded)
        batch_labels = torch.tensor(padded_labels, dtype=torch.long)

        # ëª…ì‹œì ìœ¼ë¡œ input_valuesì™€ labelsë§Œ ë°˜í™˜
        result = {
            "input_values": batch_input_values,
            "labels": batch_labels,
        }

        return result


# =====================
# ë¡œê¹… Callback
# =====================
class DetailedLoggingCallback(TrainerCallback):
    """ìƒì„¸ ë¡œê¹…"""

    def __init__(self, log_dir: str, phase_name: str):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.log_file = self.log_dir / f"{phase_name}_training.log"
        self.phase_name = phase_name
        self.start_time = None
        self.training_logs = []
        self.eval_logs = []

    def on_train_begin(self, args, state, control, **kwargs):
        self.start_time = datetime.now()
        with open(self.log_file, 'w', encoding='utf-8') as f:
            f.write(f"# {self.phase_name} Training Log\n\n")
            f.write(f"**ì‹œì‘ ì‹œê°„**: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            if 'loss' in logs:
                self.training_logs.append({
                    'step': state.global_step,
                    'loss': logs['loss'],
                    'learning_rate': logs.get('learning_rate', 0)
                })

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics:
            self.eval_logs.append({
                'step': state.global_step,
                'eval_loss': metrics.get('eval_loss', 0),
                'metrics': metrics
            })

            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(f"\n### Step {state.global_step}\n")
                f.write(f"- Eval Loss: {metrics.get('eval_loss', 0):.4f}\n")
                for k, v in metrics.items():
                    if k != 'eval_loss':
                        f.write(f"- {k}: {v}\n")

    def on_train_end(self, args, state, control, **kwargs):
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()

        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(f"\n---\n\n")
            f.write(f"**ì¢…ë£Œ ì‹œê°„**: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ì´ ì‹œê°„**: {int(duration//3600)}h {int((duration%3600)//60)}m\n\n")

            if self.eval_logs:
                best_loss = min(log['eval_loss'] for log in self.eval_logs)
                f.write(f"**ìµœê³  Eval Loss**: {best_loss:.4f}\n")


# =====================
# LoRA ì ìš©
# =====================
def apply_lora_to_model(model, config):
    """ëª¨ë¸ì— LoRA ì ìš©"""

    lora_config = LoraConfig(
        r=config['lora_r'],
        lora_alpha=config['lora_alpha'],
        target_modules=["q_proj", "k_proj", "v_proj"],
        lora_dropout=config['lora_dropout'],
        bias="none",
        # TaskType ì œê±° - Wav2Vec2ForCTCëŠ” íŠ¹ë³„í•œ task_type ë¶ˆí•„ìš”
    )

    model = get_peft_model(model, lora_config)

    print(f"\nâœ… LoRA ì ìš© ì™„ë£Œ")
    model.print_trainable_parameters()

    return model


def freeze_layers(model, freeze_layer_indices):
    """íŠ¹ì • ì¸µ ë™ê²°"""
    for i in freeze_layer_indices:
        for param in model.wav2vec2.encoder.layers[i].parameters():
            param.requires_grad = False

    print(f"âœ… {len(freeze_layer_indices)}ê°œ ì¸µ ë™ê²° ì™„ë£Œ (0~{max(freeze_layer_indices)})")


# =====================
# Phase ì‹¤í–‰ í•¨ìˆ˜
# =====================
def run_phase(
    phase_num: int,
    resume_from: Optional[str] = None,
    config: Dict = None,
):
    """ë‹¨ì¼ Phase ì‹¤í–‰"""

    if config is None:
        config = COMMON_CONFIG

    phase_config = PHASE_CONFIG[phase_num]
    phase_name = phase_config['name']

    print("=" * 80)
    print(f"ğŸš€ Phase {phase_num} ì‹œì‘: {phase_name}")
    print("=" * 80)

    # GPU ì„¤ì •
    os.environ['CUDA_VISIBLE_DEVICES'] = config['gpu']
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path(config['output_base_dir']) / phase_name
    output_dir.mkdir(parents=True, exist_ok=True)

    # Processor ë¡œë“œ
    print("\nğŸ“¦ Processor ë¡œë“œ ì¤‘...")
    processor = Wav2Vec2Processor.from_pretrained(config['model_path'])
    g2p = KoreanG2P()

    # ëª¨ë¸ ë¡œë“œ
    print("\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
    # Phase 2+ëŠ” Trainer.train()ì—ì„œ resume_from_checkpointë¡œ ë¡œë“œ
    # ì—¬ê¸°ì„œëŠ” í•­ìƒ ë² ì´ìŠ¤ ëª¨ë¸ë§Œ ë¡œë“œ
    model = Wav2Vec2ForCTC.from_pretrained(
        config['model_path'],
        attention_dropout=0.1,
        hidden_dropout=0.1,
        feat_proj_dropout=0.0,
        mask_time_prob=0.05,
        layerdrop=0.1,
        ctc_loss_reduction="mean",
        pad_token_id=processor.tokenizer.pad_token_id,
        vocab_size=len(processor.tokenizer),
    )

    # LoRA ì ìš© (Phase 1ë§Œ, Phase 2+ëŠ” ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ)
    if phase_num == 1 or resume_from:
        if resume_from:
            print(f"  Phase {phase_num}: LoRA ì„¤ì • ì ìš© (ê°€ì¤‘ì¹˜ëŠ” ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ)")
        model = apply_lora_to_model(model, config)
        freeze_layers(model, config['freeze_layers'])

    # Feature Extractor ë™ê²°
    model.freeze_feature_encoder()

    # Phase 4ëŠ” ì „ì²´ í•´ë™
    if phase_num == 4 and phase_config.get('unfreeze_all', False):
        print("\nğŸ”“ Phase 4: ì „ì²´ ëª¨ë¸ í•´ë™")
        for param in model.parameters():
            param.requires_grad = True

    model.to(device)

    # ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"\nğŸ“Š ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘... (Train ratio: {phase_config['train_ratio']:.1%})")

    train_dataset = StreamingChildSpeechDataset(
        data_dir=config['data_dir'],
        split="1.Training",
        processor=processor,
        g2p=g2p,
        train_ratio=phase_config['train_ratio'],
    )

    val_dataset = StreamingChildSpeechDataset(
        data_dir=config['data_dir'],
        split="2.Validation_split",
        processor=processor,
        g2p=g2p,
    )

    # Data Collator
    data_collator = DataCollatorCTCWithPadding(processor=processor)

    # max_steps ê³„ì‚° (IterableDatasetìš©)
    num_train_samples = len(train_dataset.json_files)
    steps_per_epoch = num_train_samples // (config['batch_size'] * config['gradient_accumulation_steps'])
    max_steps = steps_per_epoch * phase_config['epochs']

    print(f"\nğŸ“ˆ í•™ìŠµ ìŠ¤í… ê³„ì‚°:")
    print(f"  Train ìƒ˜í”Œ: {num_train_samples:,}ê°œ")
    print(f"  Steps per epoch: {steps_per_epoch:,}")
    print(f"  Total max_steps: {max_steps:,}")

    # Training Arguments
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        per_device_train_batch_size=config['batch_size'],
        per_device_eval_batch_size=config['batch_size'],
        gradient_accumulation_steps=config['gradient_accumulation_steps'],
        eval_strategy="steps",
        eval_steps=500,
        save_steps=500,
        logging_steps=50,
        learning_rate=phase_config['learning_rate'],
        warmup_steps=phase_config['warmup_steps'],
        max_steps=max_steps,
        weight_decay=config['weight_decay'],
        max_grad_norm=config['gradient_clip'],
        fp16=True,
        dataloader_num_workers=config['num_workers'],
        dataloader_pin_memory=config['pin_memory'],
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        save_total_limit=2,
        report_to="none",
        remove_unused_columns=True,
    )

    # Callbacks
    callbacks = [
        EarlyStoppingCallback(
            early_stopping_patience=config['early_stopping_patience'],
            early_stopping_threshold=config['early_stopping_threshold'],
        ),
        DetailedLoggingCallback(
            log_dir=str(output_dir),
            phase_name=phase_name,
        ),
    ]

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        callbacks=callbacks,
    )

    # í•™ìŠµ ì‹œì‘
    print(f"\nğŸƒ Phase {phase_num} í•™ìŠµ ì‹œì‘!")
    print(f"  ëª©í‘œ WER: < {phase_config['target_wer']*100:.0f}%")
    print(f"  Epochs: {phase_config['epochs']}")
    print(f"  Learning Rate: {phase_config['learning_rate']}")
    if resume_from:
        print(f"  ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: {resume_from}")
    print("-" * 80)

    # Phase 2+ëŠ” resume_from_checkpoint ì‚¬ìš©
    if resume_from:
        trainer.train(resume_from_checkpoint=resume_from)
    else:
        trainer.train()

    # ìµœì¢… ì €ì¥
    final_model_path = output_dir / "final_model"
    trainer.save_model(str(final_model_path))
    processor.save_pretrained(str(final_model_path))

    print(f"\nâœ… Phase {phase_num} ì™„ë£Œ!")
    print(f"  ëª¨ë¸ ì €ì¥: {final_model_path}")
    print(f"  Best ëª¨ë¸: {output_dir / 'checkpoint-best'}")
    print("=" * 80 + "\n")

    return str(output_dir / "checkpoint-best")


# =====================
# ë©”ì¸ ì‹¤í–‰
# =====================
def main():
    parser = argparse.ArgumentParser(description="Wav2Vec2 Multi-Phase Training")
    parser.add_argument(
        '--phase',
        type=str,
        default='1',
        help='ì‹¤í–‰í•  Phase: 1, 2, 3, 4, or all (ê¸°ë³¸ê°’: 1)'
    )
    parser.add_argument(
        '--resume_from',
        type=str,
        default=None,
        help='ì¬ê°œí•  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (Phase 2, 3, 4 ì‹œì‘ ì‹œ)'
    )
    parser.add_argument(
        '--gpu',
        type=str,
        default='3',
        help='ì‚¬ìš©í•  GPU ID (ê¸°ë³¸ê°’: 3)'
    )

    args = parser.parse_args()

    # GPU ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
    COMMON_CONFIG['gpu'] = args.gpu

    print("\n" + "=" * 80)
    print("ğŸ¯ Wav2Vec2-XLS-R-300M ì–´ë¦°ì´ ìŒì„± íŒŒì¸íŠœë‹")
    print("=" * 80)
    print(f"ì‹¤í–‰ Phase: {args.phase}")
    print(f"GPU: {args.gpu}")
    print(f"ë°ì´í„°: {COMMON_CONFIG['data_dir']}")
    print("=" * 80 + "\n")

    if args.phase == 'all':
        # ì „ì²´ Phase ìˆœì°¨ ì‹¤í–‰
        print("ğŸ”¥ ì „ì²´ Phase ìˆœì°¨ ì‹¤í–‰ (1 â†’ 2 â†’ 3)\n")

        # Phase 1
        checkpoint_phase1 = run_phase(1, config=COMMON_CONFIG)

        # Phase 2
        checkpoint_phase2 = run_phase(2, resume_from=checkpoint_phase1, config=COMMON_CONFIG)

        # Phase 3
        checkpoint_phase3 = run_phase(3, resume_from=checkpoint_phase2, config=COMMON_CONFIG)

        print("\n" + "=" * 80)
        print("ğŸ‰ ì „ì²´ Phase ì™„ë£Œ!")
        print("=" * 80)
        print(f"Phase 1 ì²´í¬í¬ì¸íŠ¸: {checkpoint_phase1}")
        print(f"Phase 2 ì²´í¬í¬ì¸íŠ¸: {checkpoint_phase2}")
        print(f"Phase 3 ì²´í¬í¬ì¸íŠ¸: {checkpoint_phase3}")
        print("=" * 80)

    else:
        # ë‹¨ì¼ Phase ì‹¤í–‰
        phase_num = int(args.phase)

        if phase_num not in [1, 2, 3, 4]:
            print("âŒ ì˜¤ë¥˜: PhaseëŠ” 1, 2, 3, 4, ë˜ëŠ” 'all'ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
            sys.exit(1)

        if phase_num > 1 and not args.resume_from:
            print(f"âš ï¸  ê²½ê³ : Phase {phase_num}ëŠ” ì´ì „ ì²´í¬í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            print(f"   --resume_from <checkpoint_path> ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)

        checkpoint = run_phase(phase_num, resume_from=args.resume_from, config=COMMON_CONFIG)

        print(f"\nâœ… Phase {phase_num} ì™„ë£Œ!")
        print(f"ì²´í¬í¬ì¸íŠ¸: {checkpoint}")


if __name__ == "__main__":
    main()
