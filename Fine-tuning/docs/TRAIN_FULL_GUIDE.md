# ğŸš€ ì „ì²´ ë°ì´í„°(3500h) LoRA í•™ìŠµ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

**ì „ì²´ 1,771,800ê°œ ìƒ˜í”Œ**ë¡œ Wav2Vec2-Korean-Phoneme ëª¨ë¸ì„ LoRA íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤.

**ë°ì´í„° ë¶„í¬:**
- Training: 1,771,800ê°œ (ì „ì²´ ì‚¬ìš©)
- Validation: 114,539ê°œ (358ëª… í™”ì)
- Test: 100,660ê°œ (358ëª… í™”ì)
- **í™”ì ì¤‘ë³µ: 0ëª…** âœ…

---

## ğŸ¯ í•™ìŠµ ì„¤ì •

### Phase 1: Full Data (3500h)

```python
{
    'name': 'phase1_full_3500h',
    'epochs': 3,
    'learning_rate': 5e-4,
    'batch_size': 16,
    'warmup_steps': 2000,
    'target_per': 12%,
}
```

### LoRA ì„¤ì •
- **r**: 16
- **alpha**: 32
- **target_modules**: q_proj, k_proj, v_proj
- **freeze_layers**: 0-7ì¸µ (í•˜ìœ„ 8ì¸µ ë™ê²°)

### ì˜ˆìƒ ì‹œê°„
- **ì´ Steps**: ~332,000 steps (3 epochs)
- **ì˜ˆìƒ ì‹œê°„**: 10-14ì¼ (GPU 3 ê¸°ì¤€)
- **1 epoch**: ~3.5-4.5ì¼

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1ï¸âƒ£ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (ê¶Œì¥)

```bash
cd /home/j-k13a206/fine_tunining_new

# nohupìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
nohup python3 train_all_phases_all.py \
  --phase 1 \
  --gpu 3 > training_full_3500h.log 2>&1 &

# í”„ë¡œì„¸ìŠ¤ ID í™•ì¸
echo $!
```

### 2ï¸âƒ£ ë¡œê·¸ ëª¨ë‹ˆí„°ë§

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸
tail -f training_full_3500h.log

# í•™ìŠµ ì§„í–‰ë¥  í™•ì¸ (grepìœ¼ë¡œ í•„í„°ë§)
tail -f training_full_3500h.log | grep -E "loss|eval"

# GPU ì‚¬ìš©ë¥  í™•ì¸
watch -n 1 nvidia-smi
```

### 3ï¸âƒ£ í•™ìŠµ ìƒíƒœ í™•ì¸

```bash
# ì²´í¬í¬ì¸íŠ¸ í™•ì¸
ls -lh checkpoints_full/phase1_full_3500h/

# ìµœì‹  ì²´í¬í¬ì¸íŠ¸
ls -lt checkpoints_full/phase1_full_3500h/ | head -10

# ë¡œê·¸ íŒŒì¼ í™•ì¸
cat checkpoints_full/phase1_full_3500h/phase1_full_3500h_training.log
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ì§€í‘œ

### í™•ì¸í•  ì§€í‘œ:
1. **Train Loss**: ê°ì†Œ ì¶”ì„¸ (ëª©í‘œ: < 0.5)
2. **Eval Loss**: ê°ì†Œ ì¶”ì„¸ (ëª©í‘œ: < 0.3)
3. **Learning Rate**: Warmup í›„ ì•ˆì •í™”
4. **Steps/sec**: ~1-2 steps/sec

### ì •ìƒ í•™ìŠµ ì˜ˆì‹œ:
```
Step 500   | Train Loss: 1.52 | Eval Loss: 1.35 | LR: 2.5e-4
Step 1000  | Train Loss: 1.12 | Eval Loss: 0.98 | LR: 5e-4
Step 5000  | Train Loss: 0.65 | Eval Loss: 0.58 | LR: 5e-4
Step 10000 | Train Loss: 0.42 | Eval Loss: 0.39 | LR: 5e-4
```

---

## ğŸ› ï¸ ì¤‘ê°„ í‰ê°€ (Optional)

í•™ìŠµ ì¤‘ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ë¡œ ì„±ëŠ¥ í™•ì¸:

```bash
# 10,000 step ì²´í¬í¬ì¸íŠ¸ë¡œ í‰ê°€
python3 evaluate_test.py \
  --model checkpoints_full/phase1_full_3500h/checkpoint-10000 \
  --test_dir /home/j-k13a206/data/child_extracted/3.Test \
  --gpu 3 \
  --output results/checkpoint_10k_results.json
```

---

## âš ï¸ ë¬¸ì œ í•´ê²°

### 1. CUDA Out of Memory

**ì¦ìƒ**: RuntimeError: CUDA out of memory

**í•´ê²°ì±…**:
```python
# train_all_phases_all.py ìˆ˜ì •
COMMON_CONFIG = {
    'batch_size': 8,  # 16 â†’ 8
    'gradient_accumulation_steps': 2,  # 1 â†’ 2
}
```

### 2. í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼

**ì¦ìƒ**: 1 step > 5ì´ˆ

**ì›ì¸**: ë°ì´í„° ë¡œë”© ë³‘ëª©

**í•´ê²°ì±…**: ì´ë¯¸ ìµœì í™”ë¨ (num_workers=0, IterableDataset)

### 3. Lossê°€ ì¤„ì§€ ì•ŠìŒ

**ì¦ìƒ**: 5000 steps í›„ì—ë„ Loss > 1.0

**í•´ê²°ì±…**:
- Learning rate ì¤„ì´ê¸°: 5e-4 â†’ 3e-4
- Warmup steps ëŠ˜ë¦¬ê¸°: 2000 â†’ 3000
- ë°ì´í„° í’ˆì§ˆ ì¬í™•ì¸

### 4. í•™ìŠµ ì¤‘ë‹¨ ë° ì¬ê°œ

**ì¤‘ë‹¨ëœ ê²½ìš°**:
```bash
# ìµœì‹  ì²´í¬í¬ì¸íŠ¸ í™•ì¸
ls -lt checkpoints_full/phase1_full_3500h/checkpoint-* | head -1

# í•´ë‹¹ ì²´í¬í¬ì¸íŠ¸ë¶€í„° ì¬ê°œ
python3 train_all_phases_all.py \
  --phase 1 \
  --resume_from checkpoints_full/phase1_full_3500h/checkpoint-XXXXX \
  --gpu 3
```

---

## âœ… ìµœì¢… í‰ê°€

í•™ìŠµ ì™„ë£Œ í›„ Test ì…‹ìœ¼ë¡œ ìµœì¢… í‰ê°€:

```bash
cd /home/j-k13a206/fine_tunining_new

python3 evaluate_test.py \
  --model checkpoints_full/phase1_full_3500h/final_model \
  --test_dir /home/j-k13a206/data/child_extracted/3.Test \
  --gpu 3 \
  --output results/full_3500h_final_results.json
```

**ëª©í‘œ ì„±ëŠ¥:**
- **PER < 12%** â­
- **CER < 8%**

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ ë¹„êµ

| ëª¨ë¸ | ë°ì´í„° | PER | CER |
|------|--------|-----|-----|
| Baseline | 0 | 36.67% | 26.40% |
| Fine-tuned (148h) | 148h | 20.55% | 15.14% |
| **Fine-tuned (3500h)** | **1.7M** | **10-12%** | **6-8%** |

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‹¤í–‰ ì „:
- [x] Validation ë¶„í•  ì™„ë£Œ (114,539ê°œ)
- [x] Test ë¶„í•  ì™„ë£Œ (100,660ê°œ)
- [x] í™”ì ì¤‘ë³µ í™•ì¸ (0ëª…)
- [ ] GPU ë©”ëª¨ë¦¬ í™•ì¸ (>= 16GB)
- [ ] ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (>= 500GB)
- [ ] `train_all_phases_all.py` ê²½ë¡œ í™•ì¸

### ì‹¤í–‰ ì¤‘:
- [ ] ë¡œê·¸ íŒŒì¼ ëª¨ë‹ˆí„°ë§
- [ ] GPU ì‚¬ìš©ë¥  í™•ì¸ (80-90%)
- [ ] Train/Eval Loss ê°ì†Œ í™•ì¸
- [ ] ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í™•ì¸ (500 stepsë§ˆë‹¤)

### ì™„ë£Œ í›„:
- [ ] Final model ì €ì¥ í™•ì¸
- [ ] Test ì…‹ í‰ê°€ ì™„ë£Œ
- [ ] PER < 12% ë‹¬ì„±
- [ ] ê²°ê³¼ JSON ì €ì¥

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

### ì„±ê³µ ì‹œ (PER < 12%):
1. âœ… ì „ì²´ ë°ì´í„° í•™ìŠµ ì™„ë£Œ!
2. ğŸ“Š ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ì‘ì„±
3. ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„

### ì‹¤íŒ¨ ì‹œ (PER > 15%):
1. Epoch ëŠ˜ë¦¬ê¸° (3 â†’ 5)
2. Learning rate ì¡°ì •
3. ë°ì´í„° í’ˆì§ˆ ì¬ê²€í† 

---

**Good luck! ğŸš€**

ì•½ 2ì£¼ í›„ ì¢‹ì€ ê²°ê³¼ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤!
