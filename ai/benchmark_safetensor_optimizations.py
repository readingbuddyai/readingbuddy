"""
SafeTensor ìµœì í™” ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬
ê° ìµœì í™” ê¸°ë²•ë³„ë¡œ ì†ë„ í–¥ìƒì„ ì¸¡ì •í•©ë‹ˆë‹¤.

ìµœì í™” ê¸°ë²•:
1. Baseline (í˜„ì¬ êµ¬í˜„)
2. Batch Inference (ë°°ì¹˜ ì¶”ë¡ )
3. Mixed Precision FP16 (ë°˜ì •ë°€ë„)
4. TorchScript JIT Compile (JIT ì»´íŒŒì¼)
5. Combined (ëª¨ë“  ìµœì í™” í†µí•©)
"""

import time
import numpy as np
import torch
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
from pathlib import Path
from typing import List, Dict
import warnings
warnings.filterwarnings("ignore")

# ì„¤ì •
MODEL_DIR = Path("models/slplab_wav2vec2_korean")
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SAMPLE_RATE = 16000
NUM_WARMUP = 10     # 3 â†’ 10 (GPU ì™„ì „íˆ ì›Œë°ì—…)
NUM_ITERATIONS = 50 # 20 â†’ 50 (í†µê³„ì  ì‹ ë¢°ë„ í–¥ìƒ)

# í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
TEST_SCENARIOS = [
    {"name": "ë‹¨ì¼ ì²­í¬ (1ì´ˆ)", "duration": 1.0, "chunks": 1},
    {"name": "ì§§ì€ ì˜¤ë””ì˜¤ (3ì´ˆ)", "duration": 3.0, "chunks": 1},
    {"name": "ê¸´ ì˜¤ë””ì˜¤ (10ì´ˆ, 10 ì²­í¬)", "duration": 10.0, "chunks": 10},
]


def create_dummy_audio(duration_sec=1.0):
    """ë”ë¯¸ ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„±"""
    num_samples = int(SAMPLE_RATE * duration_sec)
    audio = np.random.randn(num_samples).astype(np.float32)
    return audio


def create_chunks(audio: np.ndarray, num_chunks: int):
    """ì˜¤ë””ì˜¤ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    if num_chunks == 1:
        return [audio]

    chunk_size = len(audio) // num_chunks
    chunks = []
    for i in range(num_chunks):
        start = i * chunk_size
        end = start + chunk_size if i < num_chunks - 1 else len(audio)
        chunks.append(audio[start:end])
    return chunks


# ============================================================
# 1. Baseline (í˜„ì¬ êµ¬í˜„)
# ============================================================
class BaselineInference:
    """í˜„ì¬ êµ¬í˜„ (ìµœì í™” ì—†ìŒ)"""

    def __init__(self):
        print("\n[Baseline] ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.processor = Wav2Vec2Processor.from_pretrained(str(MODEL_DIR))
        self.model = Wav2Vec2ForCTC.from_pretrained(str(MODEL_DIR))
        self.model.to(DEVICE)
        self.model.eval()
        print(f"[Baseline] ë¡œë“œ ì™„ë£Œ (Device: {DEVICE})")

    @torch.no_grad()
    def infer_single(self, waveform: np.ndarray):
        """ë‹¨ì¼ ì²­í¬ ì¶”ë¡ """
        # ì „ì²˜ë¦¬
        inputs = self.processor(waveform, sampling_rate=SAMPLE_RATE, return_tensors="pt")
        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

        # ì¶”ë¡ 
        logits = self.model(**inputs).logits

        # í›„ì²˜ë¦¬
        pred_ids = torch.argmax(logits, dim=-1)
        text = self.processor.decode(pred_ids[0])
        return text

    def infer_chunks(self, chunks: List[np.ndarray]):
        """ì—¬ëŸ¬ ì²­í¬ ìˆœì°¨ ì¶”ë¡ """
        results = []
        for chunk in chunks:
            result = self.infer_single(chunk)
            results.append(result)
        return results


# ============================================================
# 2. Batch Inference (ë°°ì¹˜ ì¶”ë¡ )
# ============================================================
class BatchInference:
    """ë°°ì¹˜ ì¶”ë¡  ìµœì í™”"""

    def __init__(self, batch_size: int = 8):
        print("\n[Batch Inference] ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.processor = Wav2Vec2Processor.from_pretrained(str(MODEL_DIR))
        self.model = Wav2Vec2ForCTC.from_pretrained(str(MODEL_DIR))
        self.model.to(DEVICE)
        self.model.eval()
        self.batch_size = batch_size
        print(f"[Batch Inference] ë¡œë“œ ì™„ë£Œ (Batch Size: {batch_size})")

    @torch.no_grad()
    def infer_chunks(self, chunks: List[np.ndarray]):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²­í¬ ì¶”ë¡ """
        all_results = []

        for i in range(0, len(chunks), self.batch_size):
            batch_chunks = chunks[i:i + self.batch_size]

            # ë°°ì¹˜ ì „ì²˜ë¦¬ (paddingìœ¼ë¡œ ê¸¸ì´ ë§ì¶¤)
            inputs = self.processor(
                batch_chunks,
                sampling_rate=SAMPLE_RATE,
                return_tensors="pt",
                padding=True
            )
            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

            # ë°°ì¹˜ ì¶”ë¡ 
            logits = self.model(**inputs).logits

            # ë°°ì¹˜ í›„ì²˜ë¦¬
            pred_ids = torch.argmax(logits, dim=-1)
            texts = self.processor.batch_decode(pred_ids)
            all_results.extend(texts)

        return all_results


# ============================================================
# 3. Mixed Precision FP16 (ë°˜ì •ë°€ë„)
# ============================================================
class FP16Inference:
    """FP16 Mixed Precision ìµœì í™”"""

    def __init__(self):
        print("\n[FP16 Inference] ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.processor = Wav2Vec2Processor.from_pretrained(str(MODEL_DIR))
        self.model = Wav2Vec2ForCTC.from_pretrained(str(MODEL_DIR))

        # FP16 ë³€í™˜ (CUDAì—ì„œë§Œ ì§€ì›)
        if DEVICE == "cuda":
            self.model = self.model.half()
            self.use_fp16 = True
            print("[FP16 Inference] FP16 ëª¨ë“œ í™œì„±í™”")
        else:
            self.use_fp16 = False
            print("[FP16 Inference] CPUì—ì„œëŠ” FP32 ì‚¬ìš©")

        self.model.to(DEVICE)
        self.model.eval()

    @torch.no_grad()
    def infer_single(self, waveform: np.ndarray):
        """ë‹¨ì¼ ì²­í¬ ì¶”ë¡ """
        inputs = self.processor(waveform, sampling_rate=SAMPLE_RATE, return_tensors="pt")

        # FP16 ë³€í™˜
        if self.use_fp16:
            inputs = {k: v.to(DEVICE).half() for k, v in inputs.items()}
        else:
            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

        logits = self.model(**inputs).logits
        pred_ids = torch.argmax(logits, dim=-1)
        text = self.processor.decode(pred_ids[0])
        return text

    def infer_chunks(self, chunks: List[np.ndarray]):
        """ì—¬ëŸ¬ ì²­í¬ ìˆœì°¨ ì¶”ë¡ """
        results = []
        for chunk in chunks:
            result = self.infer_single(chunk)
            results.append(result)
        return results


# ============================================================
# 4. TorchScript JIT Compile (JIT ì»´íŒŒì¼)
# ============================================================
class JITInference:
    """TorchScript JIT ì»´íŒŒì¼ ìµœì í™”"""

    def __init__(self):
        print("\n[JIT Inference] ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.processor = Wav2Vec2Processor.from_pretrained(str(MODEL_DIR))
        model = Wav2Vec2ForCTC.from_pretrained(str(MODEL_DIR))
        model.to(DEVICE)
        model.eval()

        # JIT ì»´íŒŒì¼ ì‹œë„
        try:
            print("[JIT Inference] TorchScript ì»´íŒŒì¼ ì¤‘...")
            dummy_input = self.processor(
                np.random.randn(16000).astype(np.float32),
                sampling_rate=SAMPLE_RATE,
                return_tensors="pt"
            )
            dummy_input = {k: v.to(DEVICE) for k, v in dummy_input.items()}

            with torch.no_grad():
                # traceëŠ” kwargsë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ wrapper ì‚¬ìš©
                class ModelWrapper(torch.nn.Module):
                    def __init__(self, model):
                        super().__init__()
                        self.model = model

                    def forward(self, input_values):
                        return self.model(input_values).logits

                wrapper = ModelWrapper(model)
                self.model = torch.jit.trace(wrapper, dummy_input['input_values'])
                print("[JIT Inference] ì»´íŒŒì¼ ì™„ë£Œ")
        except Exception as e:
            print(f"[JIT Inference] ì»´íŒŒì¼ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {e}")
            self.model = model

    @torch.no_grad()
    def infer_single(self, waveform: np.ndarray):
        """ë‹¨ì¼ ì²­í¬ ì¶”ë¡ """
        inputs = self.processor(waveform, sampling_rate=SAMPLE_RATE, return_tensors="pt")
        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

        logits = self.model(inputs['input_values'])
        pred_ids = torch.argmax(logits, dim=-1)
        text = self.processor.decode(pred_ids[0])
        return text

    def infer_chunks(self, chunks: List[np.ndarray]):
        """ì—¬ëŸ¬ ì²­í¬ ìˆœì°¨ ì¶”ë¡ """
        results = []
        for chunk in chunks:
            result = self.infer_single(chunk)
            results.append(result)
        return results


# ============================================================
# 5. Combined (ëª¨ë“  ìµœì í™” í†µí•©)
# ============================================================
class CombinedInference:
    """Batch + FP16 + JIT í†µí•©"""

    def __init__(self, batch_size: int = 8):
        print("\n[Combined Inference] ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.processor = Wav2Vec2Processor.from_pretrained(str(MODEL_DIR))
        model = Wav2Vec2ForCTC.from_pretrained(str(MODEL_DIR))

        # FP16 ë³€í™˜
        if DEVICE == "cuda":
            model = model.half()
            self.use_fp16 = True
            print("[Combined] FP16 í™œì„±í™”")
        else:
            self.use_fp16 = False

        model.to(DEVICE)
        model.eval()
        self.model = model
        self.batch_size = batch_size

        print(f"[Combined] ë¡œë“œ ì™„ë£Œ (Batch: {batch_size}, FP16: {self.use_fp16})")

    @torch.no_grad()
    def infer_chunks(self, chunks: List[np.ndarray]):
        """ë°°ì¹˜ ì¶”ë¡  (FP16 + JIT)"""
        all_results = []

        for i in range(0, len(chunks), self.batch_size):
            batch_chunks = chunks[i:i + self.batch_size]

            inputs = self.processor(
                batch_chunks,
                sampling_rate=SAMPLE_RATE,
                return_tensors="pt",
                padding=True
            )

            if self.use_fp16:
                inputs = {k: v.to(DEVICE).half() for k, v in inputs.items()}
            else:
                inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

            logits = self.model(**inputs).logits
            pred_ids = torch.argmax(logits, dim=-1)
            texts = self.processor.batch_decode(pred_ids)
            all_results.extend(texts)

        return all_results


# ============================================================
# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
# ============================================================
def benchmark_model(model, chunks: List[np.ndarray], name: str) -> Dict:
    """ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •"""
    # ì›œì—…
    for _ in range(NUM_WARMUP):
        _ = model.infer_chunks(chunks)

    # ì¸¡ì •
    times = []
    for _ in range(NUM_ITERATIONS):
        start = time.time()
        _ = model.infer_chunks(chunks)
        elapsed = time.time() - start
        times.append(elapsed)

    times = np.array(times)
    return {
        'name': name,
        'mean': np.mean(times),
        'std': np.std(times),
        'min': np.min(times),
        'max': np.max(times),
        'median': np.median(times),
    }


def print_results(results: List[Dict], scenario_name: str):
    """ê²°ê³¼ ì¶œë ¥ (í†µê³„ ì •ë³´ ê°•í™”)"""
    print(f"\n{'='*80}")
    print(f"ì‹œë‚˜ë¦¬ì˜¤: {scenario_name}")
    print(f"{'='*80}")

    baseline_time = results[0]['mean']

    print(f"\n{'ëª¨ë¸':<25} {'í‰ê·  (ms)':<12} {'í‘œì¤€í¸ì°¨':<12} {'CV%':<8} {'ìµœì†Œ':<10} {'ìµœëŒ€':<10} {'ì†ë„ í–¥ìƒ'}")
    print("-" * 90)

    for result in results:
        speedup = baseline_time / result['mean']
        speedup_text = f"{speedup:.2f}x" if result != results[0] else "-"

        # ë³€ë™ê³„ìˆ˜ (Coefficient of Variation) - ì‹ ë¢°ë„ ì§€í‘œ
        cv = (result['std'] / result['mean']) * 100

        print(f"{result['name']:<25} "
              f"{result['mean']*1000:>10.2f}ms "
              f"{result['std']*1000:>10.2f}ms "
              f"{cv:>6.1f}% "
              f"{result['min']*1000:>8.2f}ms "
              f"{result['max']*1000:>8.2f}ms "
              f"{speedup_text:>10}")

    # ì‹ ë¢°ë„ í•´ì„ ì¶”ê°€
    print(f"\nğŸ’¡ ì‹ ë¢°ë„ í•´ì„ (CV = ë³€ë™ê³„ìˆ˜):")
    print(f"   CV < 5%:  âœ… ë§¤ìš° ì•ˆì •ì ")
    print(f"   CV 5-10%: âš ï¸  ë³´í†µ")
    print(f"   CV > 10%: âŒ ë³€ë™ì„± í¼ (ì¸¡ì • íšŸìˆ˜ ëŠ˜ë¦¬ê¸° ê¶Œì¥)")


def main():
    print("="*80)
    print("SafeTensor ìµœì í™” ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("="*80)
    print(f"\nì„¤ì •:")
    print(f"  Device: {DEVICE}")
    print(f"  ì›Œë°ì—… íšŸìˆ˜: {NUM_WARMUP}")
    print(f"  ì¸¡ì • íšŸìˆ˜: {NUM_ITERATIONS}")
    print(f"  ìƒ˜í”Œë§ ë ˆì´íŠ¸: {SAMPLE_RATE}Hz")

    # ëª¨ë¸ ì´ˆê¸°í™”
    models = [
        ("1. Baseline (í˜„ì¬)", BaselineInference()),
        ("2. Batch Inference", BatchInference(batch_size=8)),
        ("3. FP16 Mixed Precision", FP16Inference()),
        ("4. TorchScript JIT", JITInference()),
        ("5. Combined (All)", CombinedInference(batch_size=8)),
    ]

    # ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ í…ŒìŠ¤íŠ¸
    for scenario in TEST_SCENARIOS:
        print(f"\n\n{'='*80}")
        print(f"í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")
        print(f"{'='*80}")

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        audio = create_dummy_audio(scenario['duration'])
        chunks = create_chunks(audio, scenario['chunks'])

        print(f"ì˜¤ë””ì˜¤ ê¸¸ì´: {scenario['duration']}ì´ˆ")
        print(f"ì²­í¬ ê°œìˆ˜: {len(chunks)}ê°œ")
        print(f"ì²­í¬ë‹¹ ìƒ˜í”Œ ìˆ˜: {[len(c) for c in chunks]}")

        # ê° ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        results = []
        for name, model in models:
            print(f"\n[í…ŒìŠ¤íŠ¸ ì¤‘] {name}...")
            try:
                result = benchmark_model(model, chunks, name)
                results.append(result)
                print(f"  ì™„ë£Œ: í‰ê·  {result['mean']*1000:.2f}ms")
            except Exception as e:
                print(f"  ì‹¤íŒ¨: {e}")

        # ê²°ê³¼ ì¶œë ¥
        if results:
            print_results(results, scenario['name'])

    print(f"\n{'='*80}")
    print("ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
