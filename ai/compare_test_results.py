"""
í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¹„êµ ë° ë¦¬í¬íŠ¸ ìƒì„±

ê¸°ëŠ¥:
1. JSON ê²°ê³¼ íŒŒì¼ ì½ê¸° ë° ë¶„ì„
2. ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¹„êµ
3. Markdown/HTML ë¦¬í¬íŠ¸ ìƒì„±
4. ìµœì  ì„¤ì • ì¶”ì²œ
"""

import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict
import numpy as np


class TestResultAnalyzer:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""

    def __init__(self):
        self.results = []

    def load_result_file(self, file_path: str):
        """ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.results.extend(data if isinstance(data, list) else [data])

    def analyze(self) -> Dict:
        """ì „ì²´ ê²°ê³¼ ë¶„ì„"""
        if not self.results:
            return {}

        analysis = {
            'total_tests': len(self.results),
            'by_method': {},
            'recommendations': []
        }

        # ë°©ë²•ë³„ í†µê³„
        for method in ['fp16', 'batch', 'combined']:
            accuracies = [r[method]['accuracy']['exact_match_rate'] for r in self.results]
            speedups = [r[method]['speedup'] for r in self.results]
            char_accuracies = [r[method]['accuracy']['char_accuracy'] for r in self.results]

            analysis['by_method'][method] = {
                'accuracy': {
                    'mean': np.mean(accuracies),
                    'std': np.std(accuracies),
                    'min': np.min(accuracies),
                    'max': np.max(accuracies)
                },
                'char_accuracy': {
                    'mean': np.mean(char_accuracies),
                    'std': np.std(char_accuracies),
                    'min': np.min(char_accuracies),
                    'max': np.max(char_accuracies)
                },
                'speedup': {
                    'mean': np.mean(speedups),
                    'std': np.std(speedups),
                    'min': np.min(speedups),
                    'max': np.max(speedups)
                }
            }

        # ì¶”ì²œ ìƒì„±
        analysis['recommendations'] = self._generate_recommendations(analysis)

        return analysis

    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """ì¶”ì²œ ì‚¬í•­ ìƒì„±"""
        recommendations = []

        # ì •í™•ë„ ì²´í¬
        for method, stats in analysis['by_method'].items():
            acc = stats['accuracy']['mean']
            speedup = stats['speedup']['mean']

            if acc >= 99.0:
                recommendations.append(
                    f"âœ… {method.upper()}: ì •í™•ë„ {acc:.1f}%, ì†ë„ {speedup:.2f}x - ì•ˆì „í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥"
                )
            elif acc >= 95.0:
                recommendations.append(
                    f"âš ï¸  {method.upper()}: ì •í™•ë„ {acc:.1f}% - ì¼ë¶€ ì°¨ì´ ìˆìŒ, ê²€ì¦ í•„ìš”"
                )
            else:
                recommendations.append(
                    f"âŒ {method.upper()}: ì •í™•ë„ {acc:.1f}% - ì‚¬ìš© ê¶Œì¥í•˜ì§€ ì•ŠìŒ"
                )

        # ìµœì  ë°©ë²• ì¶”ì²œ
        best_method = max(
            analysis['by_method'].items(),
            key=lambda x: x[1]['speedup']['mean'] * (x[1]['accuracy']['mean'] / 100)
        )

        recommendations.append(
            f"\nğŸ† ì¶”ì²œ: {best_method[0].upper()} "
            f"(ì†ë„ {best_method[1]['speedup']['mean']:.2f}x, "
            f"ì •í™•ë„ {best_method[1]['accuracy']['mean']:.1f}%)"
        )

        return recommendations

    def generate_markdown_report(self, output_file: str = None) -> str:
        """Markdown ë¦¬í¬íŠ¸ ìƒì„±"""
        if output_file is None:
            output_file = f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

        analysis = self.analyze()

        report = []
        report.append("# SafeTensor ìµœì í™” í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸\n")
        report.append(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        report.append("---\n")

        # ìš”ì•½
        report.append("## ğŸ“Š í…ŒìŠ¤íŠ¸ ìš”ì•½\n")
        report.append(f"- **ì´ í…ŒìŠ¤íŠ¸ ìˆ˜**: {analysis['total_tests']}ê°œ\n")

        for result in self.results:
            report.append(f"- **{result['audio_file']}**")
            report.append(f"  - ê¸¸ì´: {result['duration']:.2f}ì´ˆ")
            report.append(f"  - ì²­í¬: {result['num_chunks']}ê°œ\n")

        # ë°©ë²•ë³„ í†µê³„
        report.append("\n## ğŸ“ˆ ë°©ë²•ë³„ ì„±ëŠ¥\n")

        report.append("| ë°©ë²• | ì •í™•ë„ | ë¬¸ì ì •í™•ë„ | ì†ë„ í–¥ìƒ | í‰ê°€ |\n")
        report.append("|------|--------|------------|----------|------|\n")

        for method, stats in analysis['by_method'].items():
            acc = stats['accuracy']['mean']
            char_acc = stats['char_accuracy']['mean']
            speedup = stats['speedup']['mean']

            if acc >= 99.0:
                status = "âœ… ìš°ìˆ˜"
            elif acc >= 95.0:
                status = "âš ï¸ ë³´í†µ"
            else:
                status = "âŒ ì£¼ì˜"

            report.append(
                f"| {method.upper()} | "
                f"{acc:.1f}% Â± {stats['accuracy']['std']:.1f}% | "
                f"{char_acc:.1f}% | "
                f"{speedup:.2f}x Â± {stats['speedup']['std']:.2f}x | "
                f"{status} |\n"
            )

        # ìƒì„¸ ê²°ê³¼
        report.append("\n## ğŸ“ ìƒì„¸ ê²°ê³¼\n")

        for i, result in enumerate(self.results, 1):
            report.append(f"\n### í…ŒìŠ¤íŠ¸ {i}: {Path(result['audio_file']).name}\n")

            report.append("| ë°©ë²• | ì™„ì „ ì¼ì¹˜ìœ¨ | ë¬¸ì ì •í™•ë„ | ì‹œê°„ | ì†ë„ í–¥ìƒ |\n")
            report.append("|------|-----------|-----------|------|----------|\n")

            baseline_time = result['baseline']['time']
            report.append(f"| Baseline | - | - | {baseline_time:.3f}ì´ˆ | - |\n")

            for method in ['fp16', 'batch', 'combined']:
                acc = result[method]['accuracy']['exact_match_rate']
                char_acc = result[method]['accuracy']['char_accuracy']
                time_taken = result[method]['time']
                speedup = result[method]['speedup']

                report.append(
                    f"| {method.upper()} | "
                    f"{acc:.1f}% | "
                    f"{char_acc:.1f}% | "
                    f"{time_taken:.3f}ì´ˆ | "
                    f"{speedup:.2f}x |\n"
                )

            # ì°¨ì´ì 
            for method in ['fp16', 'batch', 'combined']:
                diffs = result[method]['accuracy']['differences']
                if diffs:
                    report.append(f"\n**{method.upper()} ì°¨ì´ì ** ({len(diffs)}ê°œ):\n")
                    for diff in diffs[:3]:
                        report.append(f"- ì²­í¬ {diff['chunk']}:\n")
                        report.append(f"  - Baseline: `{diff['baseline']}`\n")
                        report.append(f"  - {method.upper()}: `{diff['optimized']}`\n")
                    if len(diffs) > 3:
                        report.append(f"- ... ì™¸ {len(diffs) - 3}ê°œ\n")

        # ì¶”ì²œ ì‚¬í•­
        report.append("\n## ğŸ’¡ ì¶”ì²œ ì‚¬í•­\n")
        for rec in analysis['recommendations']:
            report.append(f"- {rec}\n")

        # ê²°ë¡ 
        report.append("\n## ğŸ¯ ê²°ë¡ \n")

        best_method = max(
            analysis['by_method'].items(),
            key=lambda x: x[1]['speedup']['mean']
        )

        report.append(f"1. **ìµœê³  ì†ë„**: {best_method[0].upper()} ({best_method[1]['speedup']['mean']:.2f}ë°°)\n")

        safest_method = max(
            analysis['by_method'].items(),
            key=lambda x: x[1]['accuracy']['mean']
        )

        report.append(f"2. **ìµœê³  ì •í™•ë„**: {safest_method[0].upper()} ({safest_method[1]['accuracy']['mean']:.1f}%)\n")

        report.append("\n### ì‹¤ì „ ì ìš© ê°€ì´ë“œ\n")
        report.append("```python\n")
        report.append("# ì§§ì€ ì˜¤ë””ì˜¤ (< 5ì´ˆ)\n")
        report.append("if audio_length < 5.0:\n")
        report.append("    use_fp16()  # ì•ˆì •ì ì´ê³  ë¹ ë¦„\n")
        report.append("\n")
        report.append("# ê¸´ ì˜¤ë””ì˜¤ (>= 5ì´ˆ)\n")
        report.append("else:\n")
        report.append("    use_combined()  # ìµœê³  ì„±ëŠ¥\n")
        report.append("```\n")

        # íŒŒì¼ ì €ì¥
        report_text = ''.join(report)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_text)

        print(f"\nâœ… ë¦¬í¬íŠ¸ ìƒì„±: {output_file}")
        return report_text

    def print_summary(self):
        """ì½˜ì†”ì— ìš”ì•½ ì¶œë ¥"""
        analysis = self.analyze()

        print("\n" + "="*80)
        print("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("="*80)

        print(f"\nì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {analysis['total_tests']}ê°œ")

        print(f"\n{'ë°©ë²•':<12} {'ì •í™•ë„':<20} {'ë¬¸ì ì •í™•ë„':<20} {'ì†ë„ í–¥ìƒ':<20}")
        print("-" * 80)

        for method, stats in analysis['by_method'].items():
            print(f"{method.upper():<12} "
                  f"{stats['accuracy']['mean']:>6.1f}% Â± {stats['accuracy']['std']:>5.1f}%   "
                  f"{stats['char_accuracy']['mean']:>6.1f}% Â± {stats['char_accuracy']['std']:>5.1f}%   "
                  f"{stats['speedup']['mean']:>6.2f}x Â± {stats['speedup']['std']:>5.2f}x")

        print("\n" + "="*80)
        print("ì¶”ì²œ ì‚¬í•­")
        print("="*80)
        for rec in analysis['recommendations']:
            print(rec)


def main():
    print("="*80)
    print("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±")
    print("="*80)

    # JSON íŒŒì¼ ì°¾ê¸°
    result_files = list(Path(".").glob("accuracy_test_results_*.json"))

    if not result_files:
        print("\nâŒ ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € 'python test_accuracy_with_real_audio.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    print(f"\në°œê²¬ëœ ê²°ê³¼ íŒŒì¼: {len(result_files)}ê°œ")
    for f in result_files:
        print(f"  - {f}")

    # ìµœì‹  íŒŒì¼ ì‚¬ìš©
    latest_file = max(result_files, key=lambda p: p.stat().st_mtime)
    print(f"\nì‚¬ìš©í•  íŒŒì¼: {latest_file}")

    # ë¶„ì„
    analyzer = TestResultAnalyzer()
    analyzer.load_result_file(str(latest_file))

    # ì½˜ì†” ì¶œë ¥
    analyzer.print_summary()

    # ë¦¬í¬íŠ¸ ìƒì„±
    analyzer.generate_markdown_report()

    print(f"\n{'='*80}")
    print("ë¶„ì„ ì™„ë£Œ!")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()
