#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ Test ì…‹ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ë¹„êµ
"""

import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# í‰ê°€í•  ëª¨ë¸ë“¤ ì •ì˜
MODELS = {
    "original": {
        "name": "ì›ë³¸ ëª¨ë¸ (wav2vec2-korean-phoneme)",
        "path": "/home/j-k13a206/models/wav2vec2-korean-phoneme",
        "output": "results_original.json"
    },
    "lora_r16_3500h": {
        "name": "LoRA r16 (3500h)",
        "path": "/home/j-k13a206/fine_tunining_new/checkpoints_full/phase1_full_3500h/final_model",
        "output": "results_lora_r16_3500h.json"
    },
    "lora_r16_3500h_early": {
        "name": "LoRA r16 (3500h, Early Stop)",
        "path": "/home/j-k13a206/fine_tunining_new/checkpoints_full/phase1_full_3500h_earlystop3/final_model",
        "output": "results_lora_r16_3500h_early.json"
    },
    "lora_r32_3500h": {
        "name": "LoRA r32 (3500h)",
        "path": "/home/j-k13a206/fine_tunining_new/checkpoints_full_r32/phase1_full_3500h_r32/final_model",
        "output": "results_lora_r32_3500h.json"
    },
    "lora_r32_3500h_early": {
        "name": "LoRA r32 (3500h, Early Stop)",
        "path": "/home/j-k13a206/fine_tunining_new/checkpoints_full_r32/phase1_full_3500h_r32_ealry3/final_model",
        "output": "results_lora_r32_3500h_early.json"
    }
}

def run_evaluation(model_id, model_info, gpu="3", test_dir=None):
    """ë‹¨ì¼ ëª¨ë¸ í‰ê°€ ì‹¤í–‰"""

    if test_dir is None:
        test_dir = "/home/j-k13a206/data/child_subset_100h/3.Test"

    model_path = model_info["path"]
    output_path = Path("/home/j-k13a206/fine_tunining_new/evaluation_results") / model_info["output"]

    print("\n" + "=" * 80)
    print(f"ğŸš€ í‰ê°€ ì‹œì‘: {model_info['name']}")
    print(f"ğŸ“‚ ëª¨ë¸ ê²½ë¡œ: {model_path}")
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    print("=" * 80)

    # ê²½ë¡œ ì¡´ì¬ í™•ì¸
    if not Path(model_path).exists():
        print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return None

    # evaluate_test.py ì‹¤í–‰
    cmd = [
        "python3",
        "/home/j-k13a206/fine_tunining_new/evaluate_test.py",
        "--model", model_path,
        "--test_dir", test_dir,
        "--gpu", gpu,
        "--output", str(output_path)
    ]

    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            encoding='utf-8'
        )

        # ì¶œë ¥ í‘œì‹œ
        print(result.stdout)
        if result.stderr:
            print("STDERR:", result.stderr)

        if result.returncode != 0:
            print(f"âŒ í‰ê°€ ì‹¤íŒ¨ (exit code: {result.returncode})")
            return None

        # ê²°ê³¼ ë¡œë“œ
        if output_path.exists():
            with open(output_path, 'r', encoding='utf-8') as f:
                results = json.load(f)
            print(f"âœ… í‰ê°€ ì™„ë£Œ: PER {results['per']*100:.2f}%")
            return results
        else:
            print(f"âŒ ê²°ê³¼ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return None

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def compare_results(all_results):
    """ëª¨ë“  ê²°ê³¼ ë¹„êµ ë° ìš”ì•½"""

    print("\n\n" + "=" * 80)
    print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
    print("=" * 80)

    # í…Œì´ë¸” í—¤ë”
    print(f"\n{'ëª¨ë¸ëª…':<40} {'PER':>10} {'CER':>10} {'ì„±ê³µ':>10} {'ì‹¤íŒ¨':>10}")
    print("-" * 80)

    # ê²°ê³¼ ì •ë ¬ (PER ê¸°ì¤€)
    sorted_results = sorted(
        all_results.items(),
        key=lambda x: x[1]['per'] if x[1] else float('inf')
    )

    best_model = None
    best_per = float('inf')

    for model_id, results in sorted_results:
        model_name = MODELS[model_id]['name']

        if results is None:
            print(f"{model_name:<40} {'N/A':>10} {'N/A':>10} {'N/A':>10} {'N/A':>10}")
            continue

        per = results['per'] * 100
        cer = results['cer'] * 100
        success = results['success_count']
        errors = results['error_count']

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¶”ì 
        if per < best_per:
            best_per = per
            best_model = model_name

        # ê²°ê³¼ ì¶œë ¥
        marker = " ğŸ†" if model_id == sorted_results[0][0] and results else ""
        print(f"{model_name:<40} {per:>9.2f}% {cer:>9.2f}% {success:>10,} {errors:>10,}{marker}")

    print("-" * 80)

    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê°•ì¡°
    if best_model:
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model}")
        print(f"   PER: {best_per:.2f}%")

        if best_per < 15:
            print(f"   âœ… ëª©í‘œ ë‹¬ì„±! (PER < 15%)")
        elif best_per < 18:
            print(f"   âš ï¸ ëª©í‘œì— ê·¼ì ‘ (PER < 18%)")
        else:
            print(f"   âŒ ëª©í‘œ ë¯¸ë‹¬ì„± (PER > 18%)")

    print("\n" + "=" * 80)

    # ì¢…í•© ë¹„êµ JSON ì €ì¥
    comparison_path = Path("/home/j-k13a206/fine_tunining_new/evaluation_results/comparison_summary.json")
    comparison_data = {
        "timestamp": datetime.now().isoformat(),
        "best_model": best_model,
        "best_per": best_per,
        "results": {
            model_id: {
                "name": MODELS[model_id]['name'],
                "per": results['per'] * 100 if results else None,
                "cer": results['cer'] * 100 if results else None,
                "success_count": results['success_count'] if results else None,
                "error_count": results['error_count'] if results else None,
            }
            for model_id, results in all_results.items()
        }
    }

    comparison_path.parent.mkdir(parents=True, exist_ok=True)
    with open(comparison_path, 'w', encoding='utf-8') as f:
        json.dump(comparison_data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥: {comparison_path}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description='ëª¨ë“  ëª¨ë¸ í‰ê°€ ë° ë¹„êµ')
    parser.add_argument('--gpu', type=str, default='3', help='GPU ë²ˆí˜¸')
    parser.add_argument('--test_dir', type=str,
                        default='/home/j-k13a206/data/child_subset_100h/3.Test',
                        help='Test ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--models', nargs='+',
                        choices=list(MODELS.keys()) + ['all'],
                        default=['all'],
                        help='í‰ê°€í•  ëª¨ë¸ ì„ íƒ (ê¸°ë³¸: all)')

    args = parser.parse_args()

    # í‰ê°€í•  ëª¨ë¸ ëª©ë¡ ê²°ì •
    if 'all' in args.models:
        models_to_eval = list(MODELS.keys())
    else:
        models_to_eval = args.models

    print("=" * 80)
    print("ğŸ¯ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
    print("=" * 80)
    print(f"ğŸ“¦ í‰ê°€í•  ëª¨ë¸ ìˆ˜: {len(models_to_eval)}ê°œ")
    print(f"ğŸ–¥ï¸  GPU: {args.gpu}")
    print(f"ğŸ“‚ Test ë””ë ‰í† ë¦¬: {args.test_dir}")
    print("=" * 80)

    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path("/home/j-k13a206/fine_tunining_new/evaluation_results")
    results_dir.mkdir(parents=True, exist_ok=True)

    # ëª¨ë“  ëª¨ë¸ í‰ê°€
    all_results = {}

    for i, model_id in enumerate(models_to_eval, 1):
        print(f"\n{'='*80}")
        print(f"ì§„í–‰: {i}/{len(models_to_eval)}")
        print(f"{'='*80}")

        model_info = MODELS[model_id]
        results = run_evaluation(
            model_id=model_id,
            model_info=model_info,
            gpu=args.gpu,
            test_dir=args.test_dir
        )

        all_results[model_id] = results

    # ê²°ê³¼ ë¹„êµ
    compare_results(all_results)

    print("\nâœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
