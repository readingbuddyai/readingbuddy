#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Wav2Vec2 í•œêµ­ì–´ ìŒì†Œ ì¸ì‹ ëª¨ë¸ íŒŒì¸íŠœë‹ (ì§„ì§œ ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
IterableDatasetìœ¼ë¡œ ë°°ì¹˜ ë‹¨ìœ„ on-the-fly ë¡œë”©
"""

import os
import sys
import json
import argparse
import glob
import numpy as np
import torch
import soundfile as sf
from dataclasses import dataclass
from typing import Dict, List, Union, Optional, Iterator
from torch.utils.data import IterableDataset, DataLoader
from transformers import (
    AutoTokenizer,
    Wav2Vec2FeatureExtractor,
    Wav2Vec2Processor,
    Wav2Vec2ForCTC,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    TrainerCallback
)
from multiprocessing import cpu_count
from datetime import datetime

from korean_g2p import KoreanG2P


# =====================
# User Config (Top-Level)
# - Edit these defaults instead of passing CLI flags.
# - `gpu` can be a string like "0" or "0,1" or a list like [0,1].
# =====================
USER_CONFIG = {
  # ============================================================
  # GPU ì„¤ì • - 1ê°œ ìµœì  (ë©€í‹° GPUëŠ” ì˜¤íˆë ¤ ëŠë¦¼!)
  # ============================================================
  "gpu": "1",

  # ============================================================
  # ë°°ì¹˜ í¬ê¸°
  # ============================================================
  "batch_size": 16,  # GPU 1ê°œ, ì´ 16

  # ============================================================
  # í•™ìŠµ ì„¤ì •
  # ============================================================
  "epochs": 5,
  "learning_rate": 5e-5,

  # ============================================================
  # ê²½ë¡œ - ìƒˆë¡œìš´ ë””ë ‰í† ë¦¬
  # ============================================================
  "extracted_dir": "/home/j-k13a206/data/child_extracted",
  "model_path": "/home/j-k13a206/models/wav2vec2-korean-phoneme",
  "output_dir": "/home/j-k13a206/finetunning/output_2025_11_05",  # ğŸ”¥ ìƒˆ ë””ë ‰í† ë¦¬

  # ============================================================
  # Trainer ì„¤ì •
  # ============================================================
  "warmup_steps": 500,
  "save_steps": 5000,  # eval_stepsì™€ ë™ì¼ (early stopping ìš”êµ¬ì‚¬í•­)
  "eval_steps": 5000,
  "logging_steps": 50,  # ìì£¼ ëª¨ë‹ˆí„°ë§

  # ============================================================
  # Dataset
  # ============================================================
  "max_val_samples": None,  # ì „ì²´ ì‚¬ìš©
  "train_subdir": "1.Training",
  "val_subdir": "2.Validation",

  # ============================================================
  # ëª¨ë¸
  # ============================================================
  "freeze_feature_encoder": True,

  # ============================================================
  # ğŸ”¥ ë°ì´í„° ë¡œë”© ìµœì í™” - CPU í’€ê°€ë™
  # ============================================================
  "num_workers": 48,  # 96ì½”ì–´ì˜ 50% (ì•ˆì •ì )
  # ë” ê³µê²©ì ìœ¼ë¡œ: 64 (96ì½”ì–´ì˜ 67%)

  "prefetch_factor": 10,  # ì¶©ë¶„í•œ ë²„í¼ë§
  # ë” ê³µê²©ì ìœ¼ë¡œ: 12 ë˜ëŠ” 16

  "pin_memory": True,
}


def _normalize_gpu_ids(gpu_spec) -> str:
    """Return a CUDA_VISIBLE_DEVICES string from various specs.

    Accepts: "0", "0,1", [0,1], (0,1), etc.
    Returns: e.g., "0" or "0,1". Empty string if invalid/None.
    """
    if gpu_spec is None:
        return ""
    if isinstance(gpu_spec, (list, tuple)):
        try:
            return ",".join(str(int(x)) for x in gpu_spec)
        except Exception:
            return ""
    # treat as string
    s = str(gpu_spec).strip()
    s = s.replace(" ", "")
    # basic validation: allow digits and commas
    if not s:
        return ""
    return s


class MarkdownLoggingCallback(TrainerCallback):
    """
    í•™ìŠµ ê³¼ì •ì„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ë¡œê¹…í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±
    """

    def __init__(self, log_file_path: str):
        """
        Args:
            log_file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (.md)
        """
        self.log_file_path = log_file_path
        self.start_time = None
        self.training_logs = []
        self.eval_logs = []

        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)

        # ì´ˆê¸° ë¡œê·¸ íŒŒì¼ ìƒì„±
        self._write_header()

    def _write_header(self):
        """ë§ˆí¬ë‹¤ìš´ í—¤ë” ì‘ì„±"""
        with open(self.log_file_path, 'w', encoding='utf-8') as f:
            f.write(f"# Wav2Vec2 Fine-tuning Log\n\n")
            f.write(f"**ì‹œì‘ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")

    def on_train_begin(self, args, state, control, **kwargs):
        """í•™ìŠµ ì‹œì‘ ì‹œ í˜¸ì¶œ"""
        self.start_time = datetime.now()

        with open(self.log_file_path, 'a', encoding='utf-8') as f:
            f.write("## í•™ìŠµ ì„¤ì •\n\n")
            f.write(f"- **í•™ìŠµë¥ **: {args.learning_rate}\n")
            f.write(f"- **ë°°ì¹˜ í¬ê¸°**: {args.per_device_train_batch_size}\n")
            f.write(f"- **ì—í­ ìˆ˜**: {args.num_train_epochs}\n")
            f.write(f"- **Warmup Steps**: {args.warmup_steps}\n")
            f.write(f"- **Save Steps**: {args.save_steps}\n")
            f.write(f"- **Eval Steps**: {args.eval_steps}\n")
            f.write(f"- **FP16**: {args.fp16}\n")
            f.write("\n---\n\n")
            f.write("## í•™ìŠµ ì§„í–‰ ë¡œê·¸\n\n")
            f.write("| Step | Epoch | Loss | Learning Rate | Time |\n")
            f.write("|------|-------|------|---------------|------|\n")

    def on_log(self, args, state, control, logs=None, **kwargs):
        """ë¡œê·¸ ë°œìƒ ì‹œ í˜¸ì¶œ"""
        if logs is None:
            return

        # í•™ìŠµ loss ë¡œê¹…
        if 'loss' in logs:
            step = state.global_step
            epoch = logs.get('epoch', 0)
            loss = logs.get('loss', 0)
            lr = logs.get('learning_rate', 0)

            elapsed = (datetime.now() - self.start_time).total_seconds()
            elapsed_str = f"{int(elapsed//3600):02d}:{int((elapsed%3600)//60):02d}:{int(elapsed%60):02d}"

            self.training_logs.append({
                'step': step,
                'epoch': epoch,
                'loss': loss,
                'lr': lr,
                'time': elapsed_str
            })

            with open(self.log_file_path, 'a', encoding='utf-8') as f:
                f.write(f"| {step} | {epoch:.2f} | {loss:.4f} | {lr:.2e} | {elapsed_str} |\n")

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        """í‰ê°€ ì‹œ í˜¸ì¶œ"""
        if metrics is None:
            return

        step = state.global_step
        eval_loss = metrics.get('eval_loss', 0)

        self.eval_logs.append({
            'step': step,
            'eval_loss': eval_loss,
            'metrics': metrics
        })

        with open(self.log_file_path, 'a', encoding='utf-8') as f:
            f.write(f"\n### í‰ê°€ ê²°ê³¼ (Step {step})\n\n")
            f.write(f"- **Eval Loss**: {eval_loss:.4f}\n")
            for key, value in metrics.items():
                if key != 'eval_loss':
                    f.write(f"- **{key}**: {value}\n")
            f.write("\n")

    def on_train_end(self, args, state, control, **kwargs):
        """í•™ìŠµ ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
        end_time = datetime.now()
        total_time = (end_time - self.start_time).total_seconds()

        # train_datasetì—ì„œ í†µê³„ ê°€ì ¸ì˜¤ê¸° (ìˆìœ¼ë©´)
        train_dataset = kwargs.get('train_dataloader', None)
        dataset_stats = None
        if train_dataset and hasattr(train_dataset.dataset, 'stats'):
            dataset_stats = train_dataset.dataset.stats

        with open(self.log_file_path, 'a', encoding='utf-8') as f:
            f.write("\n---\n\n")
            f.write("## í•™ìŠµ ì™„ë£Œ\n\n")
            f.write(f"**ì¢…ë£Œ ì‹œê°„**: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**ì´ í•™ìŠµ ì‹œê°„**: {int(total_time//3600)}ì‹œê°„ {int((total_time%3600)//60)}ë¶„ {int(total_time%60)}ì´ˆ\n\n")

            # ìµœì¢… í†µê³„
            if self.training_logs:
                final_loss = self.training_logs[-1]['loss']
                f.write(f"**ìµœì¢… Training Loss**: {final_loss:.4f}\n\n")

            if self.eval_logs:
                best_eval_loss = min(log['eval_loss'] for log in self.eval_logs)
                f.write(f"**ìµœê³  Eval Loss**: {best_eval_loss:.4f}\n\n")

            # ë°ì´í„° ë¡œë”© í†µê³„
            if dataset_stats:
                f.write("### ë°ì´í„° ë¡œë”© í†µê³„\n\n")
                total_processed = sum(dataset_stats.values())
                success = dataset_stats.get('success', 0)
                f.write(f"- **ì „ì²´ ì²˜ë¦¬ íŒŒì¼**: {total_processed:,}ê°œ\n")
                f.write(f"- **ì„±ê³µ**: {success:,}ê°œ ({success/max(total_processed,1)*100:.1f}%)\n")

                total_failures = total_processed - success
                if total_failures > 0:
                    f.write(f"- **ì‹¤íŒ¨**: {total_failures:,}ê°œ ({total_failures/max(total_processed,1)*100:.1f}%)\n")
                    f.write(f"  - RIFF ì˜¤ë¥˜: {dataset_stats.get('riff_errors', 0):,}ê°œ\n")
                    f.write(f"  - ë„ˆë¬´ ì§§ìŒ: {dataset_stats.get('too_short', 0):,}ê°œ\n")
                    f.write(f"  - ì˜ëª»ëœ ì˜¤ë””ì˜¤: {dataset_stats.get('invalid_audio', 0):,}ê°œ\n")
                    f.write(f"  - ë¹ˆ í…ìŠ¤íŠ¸: {dataset_stats.get('empty_text', 0):,}ê°œ\n")
                    f.write(f"  - ë¹ˆ ìŒì†Œ: {dataset_stats.get('empty_phonemes', 0):,}ê°œ\n")
                    f.write(f"  - ì „ì²˜ë¦¬ ì˜¤ë¥˜: {dataset_stats.get('processing_errors', 0):,}ê°œ\n")
                    f.write(f"  - ê¸°íƒ€ ì˜¤ë¥˜: {dataset_stats.get('other_errors', 0):,}ê°œ\n")
                f.write("\n")

            # Loss ê·¸ë˜í”„ìš© ë°ì´í„°
            f.write("### Training Loss ì¶”ì´\n\n")
            f.write("```\n")
            for log in self.training_logs[::10]:  # 10ê°œë§ˆë‹¤ ìƒ˜í”Œë§
                f.write(f"Step {log['step']:6d}: {log['loss']:.4f}\n")
            f.write("```\n\n")

            if self.eval_logs:
                f.write("### Evaluation Loss ì¶”ì´\n\n")
                f.write("```\n")
                for log in self.eval_logs:
                    f.write(f"Step {log['step']:6d}: {log['eval_loss']:.4f}\n")
                f.write("```\n")


class PercentProgressCallback(TrainerCallback):
    """
    í•™ìŠµ ì§„í–‰ë¥ ì„ 5% ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ íŒŒì¼ì— ê¸°ë¡.
    nohupë¡œ ì‹¤í–‰í•  ë•Œ ê°€ë²¼ìš´ ì§„í–‰ ê¸°ë¡ì„ ë‚¨ê¸°ê¸° ìœ„í•œ ìš©ë„.
    """

    def __init__(self, file_path: str, step_percent: int = 5, keep_history: bool = True):
        self.file_path = file_path
        self.step_percent = max(1, int(step_percent))
        self.keep_history = keep_history
        self.last_written = -1
        os.makedirs(os.path.dirname(self.file_path), exist_ok=True)

    def _write(self, percent: int, state):
        mode = 'a' if self.keep_history else 'w'
        with open(self.file_path, mode, encoding='utf-8') as f:
            f.write(
                f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  "
                f"step {getattr(state, 'global_step', 0)}/{getattr(state, 'max_steps', 0)}  "
                f"{percent}%\n"
            )

    def on_train_begin(self, args, state, control, **kwargs):
        self.last_written = -1
        if getattr(state, 'max_steps', 0):
            self._write(0, state)
            self.last_written = 0

    def on_step_end(self, args, state, control, **kwargs):
        max_steps = getattr(state, 'max_steps', 0) or 0
        if max_steps <= 0:
            return
        percent = int((getattr(state, 'global_step', 0) / max_steps) * 100)
        next_threshold = ((self.last_written // self.step_percent) + 1) * self.step_percent
        if percent >= next_threshold and next_threshold <= 100:
            self._write(next_threshold, state)
            self.last_written = next_threshold

    def on_train_end(self, args, state, control, **kwargs):
        if getattr(state, 'max_steps', 0) and (self.last_written < 100):
            self._write(100, state)
            self.last_written = 100

@dataclass
class DataCollatorCTCWithPadding:
    """
    CTC í•™ìŠµì„ ìœ„í•œ Data Collator
    ì˜¤ë””ì˜¤ì™€ ë ˆì´ë¸”ì„ íŒ¨ë”©í•˜ì—¬ ë°°ì¹˜ ìƒì„±
    """
    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True
    max_length: Optional[int] = None
    max_length_labels: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None
    pad_to_multiple_of_labels: Optional[int] = None

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # ì…ë ¥ê³¼ ë ˆì´ë¸”ì„ ë¶„ë¦¬
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        # ì˜¤ë””ì˜¤ íŒ¨ë”©
        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )

        # ë ˆì´ë¸” íŒ¨ë”©
        labels_batch = self.processor.tokenizer.pad(
            label_features,
            padding=self.padding,
            max_length=self.max_length_labels,
            pad_to_multiple_of=self.pad_to_multiple_of_labels,
            return_tensors="pt",
        )

        # íŒ¨ë”© í† í°ì„ -100ìœ¼ë¡œ êµì²´ (loss ê³„ì‚°ì‹œ ë¬´ì‹œ)
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        batch["labels"] = labels

        return batch


class StreamingAudioDataset(IterableDataset):
    """
    ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹
    íŒŒì¼ ê²½ë¡œë§Œ ì €ì¥í•˜ê³ , ë°°ì¹˜ ë‹¨ìœ„ë¡œ on-the-fly ë¡œë“œ
    """

    def __init__(
        self,
        file_pairs: List[tuple],
        processor: Wav2Vec2Processor,
        g2p: KoreanG2P,
        shuffle: bool = False,
        seed: int = 42
    ):
        """
        Args:
            file_pairs: (audio_path, json_path) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            processor: Wav2Vec2Processor
            g2p: KoreanG2P ê°ì²´
            shuffle: ì…”í”Œ ì—¬ë¶€
            seed: ëœë¤ ì‹œë“œ
        """
        self.file_pairs = file_pairs
        self.processor = processor
        self.g2p = g2p
        self.shuffle = shuffle
        self.seed = seed

        # í†µê³„ (ê°œì„ : ë” ìì„¸í•œ ë¶„ë¥˜)
        self.stats = {
            "success": 0,
            "riff_errors": 0,
            "too_short": 0,
            "invalid_audio": 0,  # NaN/Inf/Zero
            "empty_text": 0,
            "empty_phonemes": 0,
            "processing_errors": 0,
            "other_errors": 0
        }

    def __iter__(self) -> Iterator[Dict]:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ yield"""
        file_pairs = self.file_pairs.copy()

        # ì…”í”Œ
        if self.shuffle:
            import random
            rng = random.Random(self.seed)
            rng.shuffle(file_pairs)

        # ğŸ”¥ ë©€í‹°í”„ë¡œì„¸ì‹± worker ì§€ì›: ê° workerê°€ ë‹¤ë¥¸ ë°ì´í„° ì²˜ë¦¬
        worker_info = torch.utils.data.get_worker_info()
        if worker_info is not None:
            # Workerë³„ë¡œ ë°ì´í„° ë¶„í• 
            worker_id = worker_info.id
            num_workers = worker_info.num_workers
            # ê° workerëŠ” ìì‹ ì˜ IDì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ ì²˜ë¦¬
            file_pairs = [fp for i, fp in enumerate(file_pairs) if i % num_workers == worker_id]

        for audio_path, json_path in file_pairs:
            try:
                # JSON ë¡œë“œ
                with open(json_path, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)

                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = json_data.get('Transcription', {}).get('LabelText', '')
                if not text or len(text.strip()) == 0:
                    self.stats["empty_text"] += 1
                    continue

                # ì˜¤ë””ì˜¤ ë¡œë“œ
                audio, sr = sf.read(audio_path)

                # âœ… ì˜¤ë””ì˜¤ ê²€ì¦ 1: ìµœì†Œ ê¸¸ì´ ì²´í¬ (0.1ì´ˆ = 1600 ìƒ˜í”Œ at 16kHz)
                if len(audio) < 1600:
                    self.stats["too_short"] += 1
                    continue

                # âœ… ì˜¤ë””ì˜¤ ê²€ì¦ 2: NaN/Inf ì²´í¬
                if np.isnan(audio).any() or np.isinf(audio).any():
                    self.stats["invalid_audio"] += 1
                    continue

                # âœ… ì˜¤ë””ì˜¤ ê²€ì¦ 3: ëª¨ë“  ê°’ì´ 0ì¸ì§€ ì²´í¬
                if np.abs(audio).max() < 1e-8:
                    self.stats["invalid_audio"] += 1
                    continue

                # ìŒì†Œ ë³€í™˜
                phonemes = self.g2p.text_to_phonemes(text, apply_rules=True)

                # ìŒì†Œ ê²€ì¦
                if not phonemes or len(phonemes.strip()) == 0:
                    self.stats["empty_phonemes"] += 1
                    continue

                # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
                inputs = self.processor(
                    audio,
                    sampling_rate=sr,
                    return_tensors="pt",
                    padding=False
                )

                # âœ… ì „ì²˜ë¦¬ í›„ ê¸¸ì´ ì²´í¬
                if inputs.input_values.shape[1] < 1:
                    self.stats["processing_errors"] += 1
                    continue

                # ìŒì†Œë¥¼ í† í° IDë¡œ ë³€í™˜
                ids = self.processor.tokenizer(phonemes, add_special_tokens=False).input_ids
                if isinstance(ids, list) and len(ids) > 0 and isinstance(ids[0], list):
                    ids = ids[0]
                labels = ids

                # âœ… ë ˆì´ë¸” ê¸¸ì´ ì²´í¬
                if len(labels) < 1:
                    self.stats["processing_errors"] += 1
                    continue

                self.stats["success"] += 1

                yield {
                    "input_values": inputs.input_values[0],
                    "labels": labels
                }

            except Exception as e:
                error_msg = str(e).lower()
                if "riff" in error_msg:
                    self.stats["riff_errors"] += 1
                else:
                    self.stats["other_errors"] += 1
                continue

    def __len__(self):
        """ë°ì´í„°ì…‹ í¬ê¸° (íŒŒì¼ ê°œìˆ˜)"""
        return len(self.file_pairs)


class Wav2Vec2StreamingFinetuner:
    """Wav2Vec2 íŒŒì¸íŠœë‹ í´ë˜ìŠ¤ (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)"""

    def __init__(
        self,
        model_path: str,
        output_dir: str = "./results",
        freeze_feature_encoder: bool = True
    ):
        """
        Args:
            model_path: ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
            output_dir: ê²°ê³¼ ì €ì¥ ê²½ë¡œ
            freeze_feature_encoder: Feature Encoder ë™ê²° ì—¬ë¶€
        """
        self.model_path = model_path
        self.output_dir = output_dir
        self.freeze_feature_encoder = freeze_feature_encoder

        # G2P ì´ˆê¸°í™”
        self.g2p = KoreanG2P()

        # í”„ë¡œì„¸ì„œ ë° ëª¨ë¸ ë¡œë“œ
        print("ëª¨ë¸ ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_path)
        self.processor = Wav2Vec2Processor(
            feature_extractor=feature_extractor,
            tokenizer=tokenizer
        )
        self.model = Wav2Vec2ForCTC.from_pretrained(model_path)

        # Feature Encoder ë™ê²°
        if self.freeze_feature_encoder:
            print("Feature Encoder ë™ê²° (CTC í—¤ë“œë§Œ í•™ìŠµ)")
            self.model.freeze_feature_encoder()
            for param in self.model.wav2vec2.parameters():
                param.requires_grad = False

        # vocab ë¡œë“œ
        with open(os.path.join(model_path, 'vocab.json'), 'r') as f:
            self.vocab = json.load(f)

        print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! Vocab size: {len(self.vocab)}")

    def find_file_pairs(self, data_dirs: List[str]) -> List[tuple]:
        """
        ì••ì¶• í•´ì œëœ ë””ë ‰í† ë¦¬ì—ì„œ ì˜¤ë””ì˜¤-JSON íŒŒì¼ ìŒ ì°¾ê¸°

        Args:
            data_dirs: ë°ì´í„° ë””ë ‰í† ë¦¬ ë¦¬ìŠ¤íŠ¸

        Returns:
            (audio_path, json_path) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        file_pairs = []

        for data_dir in data_dirs:
            audio_dir = os.path.join(data_dir, "ì›ì²œë°ì´í„°")
            label_dir = os.path.join(data_dir, "ë¼ë²¨ë§ë°ì´í„°")

            # JSON íŒŒì¼ ì°¾ê¸°
            json_files = glob.glob(f"{label_dir}/**/*.json", recursive=True)

            for json_path in json_files:
                # ëŒ€ì‘í•˜ëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ìƒì„±
                rel_path = os.path.relpath(json_path, label_dir)
                audio_path = os.path.join(audio_dir, rel_path.replace('.json', '.wav'))

                if os.path.exists(audio_path):
                    file_pairs.append((audio_path, json_path))

        return file_pairs

    def create_streaming_dataset(
        self,
        data_dirs: List[str],
        max_samples: Optional[int] = None,
        shuffle: bool = False
    ) -> StreamingAudioDataset:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ ìƒì„± (íŒŒì¼ ê²½ë¡œë§Œ ì¤€ë¹„, ì‹¤ì œ ë¡œë”© ì•ˆí•¨)

        Args:
            data_dirs: ë°ì´í„° ë””ë ‰í† ë¦¬ ë¦¬ìŠ¤íŠ¸
            max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
            shuffle: ì…”í”Œ ì—¬ë¶€

        Returns:
            StreamingAudioDataset
        """
        print("íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘ ì¤‘...")
        file_pairs = self.find_file_pairs(data_dirs)

        if max_samples:
            file_pairs = file_pairs[:max_samples]

        print(f"ì´ {len(file_pairs):,}ê°œ íŒŒì¼ ë°œê²¬")
        print("âœ“ íŒŒì¼ ê²½ë¡œë§Œ ì¤€ë¹„ ì™„ë£Œ (ì‹¤ì œ ë¡œë”©ì€ í•™ìŠµ ì¤‘ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìˆ˜í–‰)\n")

        return StreamingAudioDataset(
            file_pairs=file_pairs,
            processor=self.processor,
            g2p=self.g2p,
            shuffle=shuffle
        )

    def train(
        self,
        train_dataset: StreamingAudioDataset,
        eval_dataset: Optional[StreamingAudioDataset] = None,
        num_epochs: int = 10,
        batch_size: int = 4,
        learning_rate: float = 3e-4,
        warmup_steps: int = 500,
        eval_steps: int = 100,
        save_steps: int = 500,
        logging_steps: int = 10,
        log_file_path: Optional[str] = None,
        progress_file_path: Optional[str] = None,
    ):
        """
        ëª¨ë¸ í•™ìŠµ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)

        Args:
            log_file_path: ë§ˆí¬ë‹¤ìš´ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ log/finetune_wav2vec2_streaming_part.md ì‚¬ìš©)
        """
        print("\n" + "=" * 60)
        print("í•™ìŠµ ì„¤ì •")
        print("=" * 60)
        print(f"  í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(train_dataset):,}ê°œ")
        if eval_dataset:
            print(f"  í‰ê°€ ìƒ˜í”Œ ìˆ˜: {len(eval_dataset):,}ê°œ")
        print(f"  ì—í­: {num_epochs}")
        print(f"  ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"  í•™ìŠµë¥ : {learning_rate}")
        print(f"  Feature Encoder ë™ê²°: {self.freeze_feature_encoder}")
        print("=" * 60)
        print("\nâœ¨ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: í•™ìŠµ ì‹œì‘ê³¼ ë™ì‹œì— ë°ì´í„° ë¡œë“œ!")
        print("   â†’ ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ ì—†ìŒ")
        print("   â†’ ë°°ì¹˜ ë‹¨ìœ„ë¡œ íŒŒì¼ ì½ê¸° â†’ ì „ì²˜ë¦¬ â†’ í•™ìŠµ\n")

        # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        if log_file_path is None:
            log_file_path = os.path.join(os.getcwd(), "log", "finetune_wav2vec2_streaming_part.md")

        print(f"ğŸ“ í•™ìŠµ ë¡œê·¸ ì €ì¥: {log_file_path}\n")

        # Data Collator
        data_collator = DataCollatorCTCWithPadding(
            processor=self.processor,
            padding=True
        )

        # Training Arguments
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            group_by_length=False,  # IterableDatasetì—ì„œëŠ” False
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            eval_strategy="steps" if eval_dataset else "no",
            num_train_epochs=num_epochs,
            fp16=torch.cuda.is_available(),
            save_steps=save_steps,
            eval_steps=eval_steps if eval_dataset else None,
            logging_steps=logging_steps,
            learning_rate=learning_rate,
            warmup_steps=warmup_steps,
            save_total_limit=2,
            push_to_hub=False,
            remove_unused_columns=False,
            # Early Stopping ê´€ë ¨ ì„¤ì •
            load_best_model_at_end=True if eval_dataset else False,
            metric_for_best_model="loss",
            greater_is_better=False,
            # IterableDataset ì„¤ì •
            max_steps=-1,  # epoch ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ
            # ğŸ”¥ ë°ì´í„° ë¡œë”© ìµœì í™”
            dataloader_num_workers=USER_CONFIG.get("num_workers", 0),
            dataloader_prefetch_factor=USER_CONFIG.get("prefetch_factor", 2) if USER_CONFIG.get("num_workers", 0) > 0 else None,
            dataloader_pin_memory=USER_CONFIG.get("pin_memory", True),
        )

        # Callbacks
        # Progress file (5% step logging)
        if progress_file_path is None:
            progress_file_path = os.path.join(os.getcwd(), "log", "train_progress.txt")

        callbacks = [MarkdownLoggingCallback(log_file_path), PercentProgressCallback(progress_file_path, step_percent=5, keep_history=True)]
        if eval_dataset:
            # patienceë¥¼ 15ë¡œ ì¦ê°€ (ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œëŠ” ë” ê¸´ patience í•„ìš”)
            callbacks.append(EarlyStoppingCallback(early_stopping_patience=15))

        trainer = Trainer(
            model=self.model,
            data_collator=data_collator,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.processor,
            callbacks=callbacks,
        )

        # í•™ìŠµ ì‹œì‘
        print("í•™ìŠµ ì‹œì‘...\n")
        trainer.train()

        # ëª¨ë¸ ì €ì¥
        print(f"\nëª¨ë¸ ì €ì¥: {self.output_dir}/final")
        trainer.save_model(f"{self.output_dir}/final")
        self.processor.save_pretrained(f"{self.output_dir}/final")

        # í†µê³„ ì¶œë ¥ (ê°œì„ : ë” ìì„¸í•œ ë¶„ë¥˜)
        print("\n" + "=" * 60)
        print("í•™ìŠµ ì™„ë£Œ - ë°ì´í„° ë¡œë”© í†µê³„")
        print("=" * 60)
        total_processed = sum(train_dataset.stats.values())
        print(f"ì „ì²´ ì²˜ë¦¬ íŒŒì¼: {total_processed:,}ê°œ")
        print(f"\nâœ“ ì„±ê³µ: {train_dataset.stats['success']:,}ê°œ "
              f"({train_dataset.stats['success']/max(total_processed,1)*100:.1f}%)")

        # ì‹¤íŒ¨ í†µê³„
        total_failures = total_processed - train_dataset.stats['success']
        if total_failures > 0:
            print(f"\nâœ— ì‹¤íŒ¨: {total_failures:,}ê°œ ({total_failures/max(total_processed,1)*100:.1f}%)")
            print(f"  - RIFF ì˜¤ë¥˜: {train_dataset.stats['riff_errors']:,}ê°œ")
            print(f"  - ë„ˆë¬´ ì§§ìŒ (<0.1ì´ˆ): {train_dataset.stats['too_short']:,}ê°œ")
            print(f"  - ì˜ëª»ëœ ì˜¤ë””ì˜¤ (NaN/Inf/Zero): {train_dataset.stats['invalid_audio']:,}ê°œ")
            print(f"  - ë¹ˆ í…ìŠ¤íŠ¸: {train_dataset.stats['empty_text']:,}ê°œ")
            print(f"  - ë¹ˆ ìŒì†Œ: {train_dataset.stats['empty_phonemes']:,}ê°œ")
            print(f"  - ì „ì²˜ë¦¬ ì˜¤ë¥˜: {train_dataset.stats['processing_errors']:,}ê°œ")
            print(f"  - ê¸°íƒ€ ì˜¤ë¥˜: {train_dataset.stats['other_errors']:,}ê°œ")
        print("=" * 60)

        print(f"\nâœ“ í•™ìŠµ ì™„ë£Œ!")
        print(f"ğŸ“Š ë¡œê·¸ íŒŒì¼: {log_file_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹± (USER_CONFIGë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©)
    parser = argparse.ArgumentParser(
        description='Wav2Vec2 í•œêµ­ì–´ ì–´ë¦°ì´ ìŒì„± íŒŒì¸íŠœë‹ (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument('--gpu', type=str, default=USER_CONFIG["gpu"],
                        help='ì‚¬ìš©í•  GPU ë²ˆí˜¸ (ì˜ˆ: 0 ë˜ëŠ” 1,2,3)')
    parser.add_argument('--batch_size', type=int, default=USER_CONFIG["batch_size"],
                        help='ë°°ì¹˜ í¬ê¸° (GPUë‹¹)')
    parser.add_argument('--epochs', type=int, default=USER_CONFIG["epochs"],
                        help='í•™ìŠµ ì—í­ ìˆ˜')
    parser.add_argument('--lr', type=float, default=USER_CONFIG["learning_rate"],
                        help='í•™ìŠµë¥ ')
    parser.add_argument('--extracted_dir', type=str, default=USER_CONFIG["extracted_dir"],
                        help='ì••ì¶• í•´ì œëœ ë°ì´í„° ë””ë ‰í† ë¦¬')
    parser.add_argument('--model_path', type=str, default=USER_CONFIG["model_path"],
                        help='ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ')
    parser.add_argument('--output_dir', type=str, default=USER_CONFIG["output_dir"],
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--warmup_steps', type=int, default=USER_CONFIG["warmup_steps"],
                        help='Warmup steps')
    parser.add_argument('--save_steps', type=int, default=USER_CONFIG["save_steps"],
                        help='ëª¨ë¸ ì €ì¥ ì£¼ê¸° (steps)')
    parser.add_argument('--eval_steps', type=int, default=USER_CONFIG["eval_steps"],
                        help='í‰ê°€ ì£¼ê¸° (steps)')
    parser.add_argument('--logging_steps', type=int, default=USER_CONFIG["logging_steps"],
                        help='ë¡œê¹… ì£¼ê¸° (steps)')
    parser.add_argument('--max_val_samples', type=int, default=USER_CONFIG["max_val_samples"],
                        help='Validation ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (Noneì´ë©´ ì „ì²´)')
    args = parser.parse_args()

    # GPU ì„¤ì • (ê°œì„ ëœ ë¡œì§)
    gpu_str = _normalize_gpu_ids(args.gpu)
    if gpu_str:
        os.environ["CUDA_VISIBLE_DEVICES"] = gpu_str
        num_gpus = len(gpu_str.split(','))
    else:
        num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0

    print("=" * 60)
    print("Wav2Vec2 íŒŒì¸íŠœë‹ - ìŠ¤íŠ¸ë¦¬ë° ë²„ì „ (ì§„ì§œ íš¨ìœ¨ì !)")
    print("=" * 60)
    print(f"ì‚¬ìš© GPU: {gpu_str if gpu_str else 'CPU'} ({num_gpus}ê°œ)")
    print(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size} (GPUë‹¹)")
    print(f"Effective ë°°ì¹˜: {args.batch_size * max(num_gpus, 1)}")
    print(f"ì—í­: {args.epochs}")
    print(f"í•™ìŠµë¥ : {args.lr}")
    print(f"ëª¨ë¸ ê²½ë¡œ: {args.model_path}")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
    print("=" * 60)

    # ì••ì¶• í•´ì œëœ ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸
    train_dir = os.path.join(args.extracted_dir, USER_CONFIG["train_subdir"])
    val_dir = os.path.join(args.extracted_dir, USER_CONFIG["val_subdir"])

    if not os.path.exists(train_dir):
        print(f"\nâŒ ì˜¤ë¥˜: {train_dir} ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
        print(f"\në¨¼ì € ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
        print(f"  bash extract_data.sh")
        sys.exit(1)

    # Finetuner ì´ˆê¸°í™”
    finetuner = Wav2Vec2StreamingFinetuner(
        model_path=args.model_path,
        output_dir=args.output_dir,
        freeze_feature_encoder=USER_CONFIG["freeze_feature_encoder"]
    )

    # Training ë°ì´í„°ì…‹ ìƒì„± (íŒŒì¼ ê²½ë¡œë§Œ ì¤€ë¹„)
    print("\n" + "=" * 60)
    print("Training ë°ì´í„°ì…‹ ì¤€ë¹„")
    print("=" * 60)
    train_dataset = finetuner.create_streaming_dataset(
        data_dirs=[train_dir],
        max_samples=None,  # ì „ì²´ ì‚¬ìš©
        shuffle=True
    )

    # Validation ë°ì´í„°ì…‹ ìƒì„±
    print("=" * 60)
    print("Validation ë°ì´í„°ì…‹ ì¤€ë¹„")
    print("=" * 60)
    validation_dataset = finetuner.create_streaming_dataset(
        data_dirs=[val_dir],
        max_samples=args.max_val_samples,
        shuffle=False
    )

    # í•™ìŠµ
    finetuner.train(
        train_dataset=train_dataset,
        eval_dataset=validation_dataset,
        num_epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        warmup_steps=args.warmup_steps,
        save_steps=args.save_steps,
        eval_steps=args.eval_steps,
        logging_steps=args.logging_steps,
    )


if __name__ == "__main__":
    main()
